{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Wikipedia Experiments\n",
    "\n",
    "This is the post tutorial example to explore performance of gensim on wikipedia article content.\n",
    "\n",
    "Download the article data from the US mirror at your.org rather than wikimedia directly.  It will be 10x faster. http://dumps.wikimedia.your.org/enwiki/20181120/ . Should only take about 5 min total.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In shell run:\n",
    "```\n",
    "if [ ! -f enwiki-20181120-pages-articles.xml.bz2 ]\n",
    "then\n",
    "   wget http://dumps.wikimedia.your.org/enwiki/20181120/enwiki-20181120-pages-articles.xml.bz2\n",
    "fi\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to parse wikipedia\n",
    "#!python -m gensim.scripts.make_wiki enwiki-20181120-pages-articles.xml.bz2 `pwd`/wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import logging, gensim\n",
    ">>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> # load id->word mapping (the dictionary), one of the results of step 2 above\n",
    ">>> id2word = gensim.corpora.Dictionary.load_from_text('wiki_wordids.txt.bz2')\n",
    ">>> # load corpus iterator\n",
    ">>> mm = gensim.corpora.MmCorpus('wiki_tfidf.mm')\n",
    ">>> # mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm.bz2') # use this if you compressed the TFIDF output\n",
    "\n",
    ">>> print(mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LDA of corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> # extract 100 LDA topics, using 1 pass and updating once every 1 chunk (10,000 documents)\n",
    ">>> lda = gensim.models.ldamodel.LdaModel(corpus=mm, id2word=id2word, num_topics=100, update_every=1, chunksize=10000, passes=1, distributed=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
