{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Corpus, Dictionary, and LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import logging, gensim\n",
    ">>> logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> # load id->word mapping (the dictionary), one of the results of step 2 above\n",
    ">>> id2word = gensim.corpora.Dictionary.load_from_text('wiki_wordids.txt.bz2')\n",
    ">>> # load corpus iterator\n",
    ">>> mm = gensim.corpora.MmCorpus('wiki_tfidf.mm')\n",
    ">>> # mm = gensim.corpora.MmCorpus('wiki_en_tfidf.mm.bz2') # use this if you compressed the TFIDF output\n",
    "\n",
    ">>> print(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = gensim.models.LdaMulticore.load(\"wiki_ldamodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore topics\n",
    "\n",
    "confirm we have the same state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage_POStagging=\"\"\"\n",
    "Homepage\n",
    "Becoming Human: Artificial Intelligence Magazine\n",
    "\n",
    "    HomeðŸ”¥ CONSULTINGðŸŽ“ TUTORIALSâœï¸ SUBMIT AN ARTICLEðŸ˜Ž COMMUNITIESðŸ¤– OUR BOT\n",
    "\n",
    "Go to the profile of Cdiscount Data Science\n",
    "Cdiscount Data Science\n",
    "We are the data science team at Cdiscount, France's largest non-food e-commerce company.\n",
    "Mar 27\n",
    "Part-of-Speech tagging tutorial with the Keras Deep Learning library\n",
    "In this tutorial, you will see how you can use a simple Keras model to train and evaluate an artificial neural network for multi-class classification problems.\n",
    "\n",
    "by Axel Bellec (Data Scientist at Cdiscount)\n",
    "Photo by Joao Tzanno on Unsplash\n",
    "\n",
    "Part-of-Speech tagging is a well-known task in Natural Language Processing. It refers to the process of classifying words into their parts of speech (also known as words classes or lexical categories). This is a supervised learning approach.\n",
    "\n",
    "Artificial neural networks have been applied successfully to compute POS tagging with great performance. We will focus on the Multilayer Perceptron Network, which is a very popular network architecture, considered as the state of the art on Part-of-Speech tagging problems.\n",
    "\n",
    "Letâ€™s put it into practice!\n",
    "\n",
    "In this post you will get a quick tutorial on how to implement a simple Multilayer Perceptron in Keras and train it on an annotated corpus.\n",
    "Ensuring reproducibility\n",
    "\n",
    "In order to be sure that our experiences can be achieved again we need to fix the random seed for reproducibility:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "CUSTOM_SEED = 42\n",
    "np.random.seed(CUSTOM_SEED)\n",
    "\n",
    "Getting an annotated corpus\n",
    "\n",
    "The Penn Treebank is an annotated corpus of POS tags. A sample is available in the NLTK python library which contains a lot of corpora that can be used to train and test some NLP models.\n",
    "\n",
    "First of all, we download the annotated corpus:\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('treebank')\n",
    "\n",
    "Then we load the tagged sentencesâ€¦\n",
    "\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "sentences = treebank.tagged_sents(tagset='universal')\n",
    "\n",
    "â€¦ and visualize one:\n",
    "\n",
    "import random\n",
    "\n",
    "print(random.choice(sentences))\n",
    "\n",
    "This yields a list of tuples (term, tag).\n",
    "\n",
    "[('Mr.', 'NOUN'), ('Otero', 'NOUN'), (',', '.'), ('who', 'PRON'), ('apparently', 'ADV'), ('has', 'VERB'), ('an', 'DET'), ('unpublished', 'ADJ'), ('number', 'NOUN'), (',', '.'), ('also', 'ADV'), ('could', 'VERB'), (\"n't\", 'ADV'), ('be', 'VERB'), ('reached', 'VERB'), ('.', '.')]\n",
    "\n",
    "This is a multi-class classification problem with more than forty different classes.]\n",
    "Top 3 Most Popular Ai Articles:\n",
    "\n",
    "    1. TensorFlow Object Detection API tutorial\n",
    "\n",
    "    2. Deep Learning Book Notes, Chapter 1\n",
    "\n",
    "    3. Deep Learning Book Notes, Chapter 2\n",
    "\n",
    "POS tagging on Treebank corpus is a well-known problem and we can expect to achieve a model accuracy larger than 95%.\n",
    "\n",
    "tags = set([\n",
    "    tag for sentence in treebank.tagged_sents() \n",
    "    for _, tag in sentence\n",
    "])\n",
    "print('nb_tags: %sntags: %s' % (len(tags), tags))\n",
    "\n",
    "This yields:\n",
    "\n",
    "46\n",
    "{'IN', 'VBZ', '.', 'RP', 'DT', 'VB', 'RBR', 'CC', '#', ',', 'VBP', 'WP$', 'PRP', 'JJ', \n",
    "'RBS', 'LS', 'PRP$', 'WRB', 'JJS', '``', 'EX', 'POS', 'WP', 'VBN', '-LRB-', '-RRB-', \n",
    "'FW', 'MD', 'VBG', 'TO', '$', 'NNS', 'NNPS', \"''\", 'VBD', 'JJR', ':', 'PDT', 'SYM', \n",
    "'NNP', 'CD', 'RB', 'WDT', 'UH', 'NN', '-NONE-'}\n",
    "\n",
    "Datasets preprocessing for supervised learning\n",
    "\n",
    "We split our tagged sentences into 3 datasets :\n",
    "\n",
    "    a training dataset which corresponds to the sample data used to fit the model,\n",
    "    a validation dataset used to tune the parameters of the classifier, for example to choose the number of units in the neural network,\n",
    "    a test dataset used only to assess the performance of the classifier.\n",
    "\n",
    "We use approximately 60% of the tagged sentences for training, 20% as the validation set and 20% to evaluate our model.\n",
    "\n",
    "train_test_cutoff = int(.80 * len(sentences)) \n",
    "training_sentences = sentences[:train_test_cutoff]\n",
    "testing_sentences = sentences[train_test_cutoff:]\n",
    "\n",
    "train_val_cutoff = int(.25 * len(training_sentences))\n",
    "validation_sentences = training_sentences[:train_val_cutoff]\n",
    "training_sentences = training_sentences[train_val_cutoff:]\n",
    "\n",
    "Feature engineering\n",
    "\n",
    "Our set of features is very simple.\n",
    "For each term we create a dictionnary of features depending on the sentence where the term has been extracted from.\n",
    "These properties could include informations about previous and next words as well as prefixes and suffixes.\n",
    "\n",
    "def add_basic_features(sentence_terms, index):\n",
    "    \\\"\"\" Compute some very basic word features.\n",
    "\n",
    "        :param sentence_terms: [w1, w2, ...] \n",
    "        :type sentence_terms: list\n",
    "        :param index: the index of the word \n",
    "        :type index: int\n",
    "        :return: dict containing features\n",
    "        :rtype: dict\n",
    "    \\\"\"\"\n",
    "    term = sentence_terms[index]\n",
    "    return {\n",
    "        'nb_terms': len(sentence_terms),\n",
    "        'term': term,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence_terms) - 1,\n",
    "        'is_capitalized': term[0].upper() == term[0],\n",
    "        'is_all_caps': term.upper() == term,\n",
    "        'is_all_lower': term.lower() == term,\n",
    "        'prefix-1': term[0],\n",
    "        'prefix-2': term[:2],\n",
    "        'prefix-3': term[:3],\n",
    "        'suffix-1': term[-1],\n",
    "        'suffix-2': term[-2:],\n",
    "        'suffix-3': term[-3:],\n",
    "        'prev_word': '' if index == 0 else sentence_terms[index - 1],\n",
    "        'next_word': '' if index == len(sentence_terms) - 1 else sentence_terms[index + 1]\n",
    "    }\n",
    "\n",
    "We map our list of sentences to a list of dict features.\n",
    "\n",
    "def untag(tagged_sentence):\n",
    "    \\\"\"\" \n",
    "    Remove the tag for each tagged term.\n",
    "\n",
    ":param tagged_sentence: a POS tagged sentence\n",
    "    :type tagged_sentence: list\n",
    "    :return: a list of tags\n",
    "    :rtype: list of strings\n",
    "    \\\"\"\"\n",
    "    return [w for w, _ in tagged_sentence]\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    \\\"\"\"\n",
    "    Split tagged sentences to X and y datasets and append some basic features.\n",
    "\n",
    ":param tagged_sentences: a list of POS tagged sentences\n",
    "    :param tagged_sentences: list of list of tuples (term_i, tag_i)\n",
    "    :return: \n",
    "    \\\"\"\"\n",
    "    X, y = [], []\n",
    "\n",
    "for pos_tags in tagged_sentences:\n",
    "        for index, (term, class_) in enumerate(pos_tags):\n",
    "            # Add basic NLP features for each sentence term\n",
    "            X.append(add_basic_features(untag(pos_tags), index))\n",
    "            y.append(class_)\n",
    "    return X, y\n",
    "\n",
    "For training, validation and testing sentences, we split the attributes into X (input variables) and y (output variables).\n",
    "\n",
    "X_train, y_train = transform_to_dataset(training_sentences)\n",
    "X_test, y_test = transform_to_dataset(testing_sentences)\n",
    "X_val, y_val = transform_to_dataset(validation_sentences)\n",
    "\n",
    "Features encoding\n",
    "\n",
    "Our neural network takes vectors as inputs, so we need to convert our dict features to vectors.\n",
    "sklearn builtin function DictVectorizer provides a straightforward way to do that.\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Fit our DictVectorizer with our set of features\n",
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "dict_vectorizer.fit(X_train + X_test + X_val)\n",
    "\n",
    "# Convert dict features to vectors\n",
    "X_train = dict_vectorizer.transform(X_train)\n",
    "X_test = dict_vectorizer.transform(X_test)\n",
    "X_val = dict_vectorizer.transform(X_val)\n",
    "\n",
    "Our y vectors must be encoded. The output variable contains 49 different string values that are encoded as integers.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fit LabelEncoder with our list of classes\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train + y_test + y_val)\n",
    "\n",
    "# Encode class values as integers\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "\n",
    "And then we need to convert those encoded values to dummy variables (one-hot encoding).\n",
    "\n",
    "# Convert integers to dummy variables (one hot encoded)\n",
    "from keras.utils import np_utils\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "y_val = np_utils.to_categorical(y_val)\n",
    "\n",
    "Building a Keras model\n",
    "\n",
    "Keras is a high-level framework for designing and running neural networks on multiple backends like TensorFlow, Theano or CNTK.\n",
    "\n",
    "We want to create one of the most basic neural networks: the Multilayer Perceptron. This kind of linear stack of layers can easily be made with the Sequential model. This model will contain an input layer, an hidden layer, and an output layer.\n",
    "To overcome overfitting, we use dropout regularization. We set the dropout rate to 20%, meaning that 20% of the randomly selected neurons are ignored during training at each update cycle.\n",
    "\n",
    "We use Rectified Linear Units (ReLU) activations for the hidden layers as they are the simplest non-linear activation functions available.\n",
    "\n",
    "For multi-class classification, we may want to convert the units outputs to probabilities, which can be done using the softmax function. We decide to use the categorical cross-entropy loss function.\n",
    "Finally, we choose Adam optimizer as it seems to be well suited to classification tasks.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "def build_model(input_dim, hidden_neurons, output_dim):\n",
    "    \\\"\"\"\n",
    "    Construct, compile and return a Keras model which will be used to fit/predict\n",
    "    \\\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(hidden_neurons, input_dim=input_dim),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_neurons),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "Creating a wrapper between Keras API and Scikit-Learn\n",
    "\n",
    "Keras provides a wrapper called KerasClassifier which implements the Scikit-Learn classifier interface.\n",
    "\n",
    "All model parameters are defined below. We need to provide a function that returns the structure of a neural network (build_fn).\n",
    "The number of hidden neurons and the batch size are choose quite arbitrarily. We set the number of epochs to 5 because with more iterations the Multilayer Perceptron starts overfitting (even with Dropout Regularization).\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model_params = {\n",
    "    'build_fn': build_model,\n",
    "    'input_dim': X_train.shape[1],\n",
    "    'hidden_neurons': 512,\n",
    "    'output_dim': y_train.shape[1],\n",
    "    'epochs': 5,\n",
    "    'batch_size': 256,\n",
    "    'verbose': 1,\n",
    "    'validation_data': (X_val, y_val),\n",
    "    'shuffle': True\n",
    "}\n",
    "\n",
    "clf = KerasClassifier(**model_params)\n",
    "\n",
    "Training our Keras model\n",
    "\n",
    "Finally, we can train our Multilayer perceptron on train dataset.\n",
    "\n",
    "hist = clf.fit(X_train, y_train)\n",
    "\n",
    "With the callback history provided we can visualize the model log loss and accuracy against time.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_model_performance(train_loss, train_acc, train_val_loss, train_val_acc):\n",
    "    \\\"\"\" Plot model loss and accuracy through epochs. \\\"\"\"\n",
    "\n",
    "    blue= '#34495E'\n",
    "    green = '#2ECC71'\n",
    "    orange = '#E23B13'\n",
    "\n",
    "    # plot model loss\n",
    "    fig, (ax1, ax2) = plt.subplots(2, figsize=(10, 8))\n",
    "    ax1.plot(range(1, len(train_loss) + 1), train_loss, blue, linewidth=5, label='training')\n",
    "    ax1.plot(range(1, len(train_val_loss) + 1), train_val_loss, green, linewidth=5, label='validation')\n",
    "    ax1.set_xlabel('# epoch')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.tick_params('y')\n",
    "    ax1.legend(loc='upper right', shadow=False)\n",
    "    ax1.set_title('Model loss through #epochs', color=orange, fontweight='bold')\n",
    "\n",
    "    # plot model accuracy\n",
    "    ax2.plot(range(1, len(train_acc) + 1), train_acc, blue, linewidth=5, label='training')\n",
    "    ax2.plot(range(1, len(train_val_acc) + 1), train_val_acc, green, linewidth=5, label='validation')\n",
    "    ax2.set_xlabel('# epoch')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.tick_params('y')\n",
    "    ax2.legend(loc='lower right', shadow=False)\n",
    "    ax2.set_title('Model accuracy through #epochs', color=orange, fontweight='bold')\n",
    "\n",
    "Then, display model performance:\n",
    "\n",
    "plot_model_performance(\n",
    "    train_loss=hist.history.get('loss', []),\n",
    "    train_acc=hist.history.get('acc', []),\n",
    "    train_val_loss=hist.history.get('val_loss', []),\n",
    "    train_val_acc=hist.history.get('val_acc', [])\n",
    ")\n",
    "\n",
    "Model performance vs. epochs\n",
    "\n",
    "After 2 epochs, we see that our model begins to overfit.\n",
    "Evaluating our multilayer perceptron\n",
    "\n",
    "Since our model is trained, we can evaluate it (compute its accuracy):\n",
    "\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)\n",
    "\n",
    "[Out] 0.95816\n",
    "\n",
    "We are pretty close to 96% accuracy on test dataset, that is quite impressive when you look at the basic features we injected in the model.\n",
    "Keep also in mind that 100% accuracy is not possible even for human annotators. We estimate humans can do Part-of-Speech tagging at about 98% accuracy.\n",
    "Visualizing the model\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(clf.model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "Save the Keras model\n",
    "\n",
    "Saving a Keras model is pretty simple as a method is provided natively:\n",
    "\n",
    "clf.model.save('/tmp/keras_mlp.h5')\n",
    "\n",
    "This saves the architecture of the model, the weights as well as the training configuration (loss, optimizer).\n",
    "Ressources\n",
    "\n",
    "    Keras: The Python Deep Learning library: [doc]\n",
    "    Adam: A Method for Stochastic Optimization: [paper]\n",
    "    Improving neural networks by preventing co-adaptation of feature detectors: [paper]\n",
    "\n",
    "In this post, you learn how to define and evaluate accuracy of a neural network for multi-class classification using the Keras library.\n",
    "The script used to illustrate this post is provided here : [.py|.ipynb].\n",
    "\n",
    "    This post was originally published on Cdiscount Techblog.\n",
    "\n",
    "    Machine LearningKerasPythonNeural NetworksArtificial Intelligence\n",
    "\n",
    "Go to the profile of Cdiscount Data Science\n",
    "Cdiscount Data Science\n",
    "\n",
    "We are the data science team at Cdiscount, France's largest non-food e-commerce company.\n",
    "Becoming Human: Artificial Intelligence Magazine\n",
    "Becoming Human: Artificial Intelligence Magazine\n",
    "\n",
    "Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.\n",
    "More from Becoming Human: Artificial Intelligence Magazine\n",
    "Variational AutoEncoders for new fruits with Keras and Pytorch.\n",
    "Go to the profile of Thomas Dehaene\n",
    "Thomas Dehaene\n",
    "More from Becoming Human: Artificial Intelligence Magazine\n",
    "The Dawn of Geometric Intelligence, Long Live the Age of Algorithmic Intelligence\n",
    "Go to the profile of Marcel Masaga\n",
    "Marcel Masaga\n",
    "More from Becoming Human: Artificial Intelligence Magazine\n",
    "Making beats with generative design\n",
    "Go to the profile of Simon Asp\n",
    "Simon Asp\n",
    "Responses\n",
    "John-Paul Robinson\n",
    "Write a responseâ€¦\n",
    "John-Paul Robinson\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> doc = webpage_POStagging\n",
    ">>> vec_bow = id2word.doc2bow(doc.lower().split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> vec_lda = lda[vec_bow] # convert the query to LSI space\n",
    ">>> print(vec_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topic(topicid=19, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.show_topic(topicid=83, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.get_document_topics(vec_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.print_topic(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.top_topics(mm)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
