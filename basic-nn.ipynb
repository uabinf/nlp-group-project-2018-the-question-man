{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to build a neural networks\n",
    "\n",
    "Many of the NLP tasks rely on sequence prediction from trained neural networks.  Understanding the fundamentals of their operation and implementation can help navigate higher-level frameworks like Keras and guide selection of meta-parameters used to build the network.\n",
    "\n",
    "This example works with a hand-coded, bare neural network implementation in Python to expose how the operations in the network are implemented.\n",
    "\n",
    "Like all learning activities, we start with a search of available resources [build a neural network from scratch](https://www.google.com/search?client=ubuntu&channel=fs&q=build+a+neural+network+from+scratch&ie=utf-8&oe=utf-8).\n",
    "\n",
    "Since we are interested in applying our networks to sequence prediction, let's see if we can train a network with one of the most basic sequences: counting.  That is, can we train a network to count.  Given input 1 our network should return 2.  Given 2, return 3. Given 3, return 4 and so on. We want our network to learning to add 1 to the input number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN First Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the basic example of building a neural network from scratch.\n",
    "\n",
    "https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output = np.zeros(y.shape)\n",
    "        \n",
    "    def feedforward(self):\n",
    "        self.layer1 = self.sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = self.sigmoid(np.dot(self.layer1, self.weights2))\n",
    "        \n",
    "    def backprop(self):\n",
    "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
    "\n",
    "        # update the weights with the derivative (slope) of the loss function\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The article does a nice job of laying out the motivations and logic behind the above implementation but then doesn't provide any code for testing and training\n",
    "\n",
    "it just shows an input data set with the desired output, which looks like an xor function but doesn't give any insight on how to train for 1500 iterrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(np.array([[0, 1, 2]]), np.array([[1, 2, 3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't even provide an implementation for the referenced `sigmoid()` function.  This leaves the [sigmoid()](https://gist.github.com/jovianlin/805189d4b19332f8b8a79bbd07e3f598) from the web erroring out on the format of the input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.feedforward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin understanding numpy arrays\n",
    "\n",
    "It's not entirely surprising that our random grab of `sigmoid()` from the web should fail with a different input in a new context.  Some portability is expected, however, given we are using numpy.  We kinda expect it to magically take care of applying the correct mathematical operations for a given input.  When it doesn't, like above, it's a good sign we aren't fully understanding our data struture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy let's us create vectors (numpy arrays) with some pretty simple syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect their sturcture wiht the shape method, in this case y has three elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a 3x4 matrix of random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some contrived vectors and their shape attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([[1, 2]])\n",
    "y=np.array([[2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And another random matrix that relies on one dimension being defined by the shape of another variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(x.shape[1],4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To multiply to 1x2 vectors (matricies) we need our inner dimensions to line up, a 1x2 and 2x1. Simply take the transpose() of the object (also the \".T\" method). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(x,y.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stright forward attempt of a 1x2 dot 1x2 won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Second  Pass\n",
    "\n",
    "The http://medium.com post didn't provide a complete implementation, leaving off details on the training step an use of the created NN object.\n",
    "\n",
    "Revisiting the search [how to build neuralnetwork in python](https://www.google.com/search?client=ubuntu&channel=fs&q=how+to+build+neuralnetwork+in+python&ie=utf-8&oe=utf-8) and looking past the first recommendation above leads to a more comprehensive example [Build a Neural Network](https://enlight.nyc/projects/neural-network/) on the http://enlight.nyc site. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example trains a network on a classic learning example, the expected test score given number of hours studied and slept as input.  That is a 2d input (hours studied and slept) leads to a 1d output (test score).\n",
    "\n",
    "Our initial pass at the implementation is slightly simplified over the given example to accomidate our expected use case, a 1d input (starting whole number) leads to a 1d output (next whole number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        #parameters\n",
    "        self.inputSize = 1\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 3\n",
    "\n",
    "\n",
    "\n",
    "        #weights\n",
    "        self.W1 = np.random.randn(self.inputSize, self.hiddenSize) # (3x2) weight matrix from input to hidden layer\n",
    "        self.W2 = np.random.randn(self.hiddenSize, self.outputSize) # (3x1) weight matrix from hidden to output layer\n",
    "        return\n",
    "        \n",
    "    def forward(self, X):\n",
    "    #forward propagation through our network\n",
    "      self.z = np.dot(X, self.W1) # dot product of X (input) and first set of 3x2 weights\n",
    "      self.z2 = self.sigmoid(self.z) # activation function\n",
    "      self.z3 = np.dot(self.z2, self.W2) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
    "      o = self.sigmoid(self.z3) # final activation function\n",
    "      return o\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def sigmoid(self, s):\n",
    "      # activation function\n",
    "      return 1/(1+np.exp(-s))\n",
    "\n",
    "\n",
    "\n",
    "    def sigmoidPrime(self, s):\n",
    "      #derivative of sigmoid\n",
    "      return s * (1 - s)\n",
    "\n",
    "\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "        # print statements introduced during debug of np.array() shape errors during weight calculation\n",
    "        #print(\"X: {0}\".format(X.shape))\n",
    "        #print(\"y: {0}\".format(y.shape))\n",
    "        #print(\"o: {0}\".format(o.shape))\n",
    "        \n",
    "        # backward propagate through the network\n",
    "        self.o_error = y - o # error in output\n",
    "        #print(\"o_error: {0}\".format(self.o_error.shape))\n",
    "        self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
    "        #print(\"o_delta: {0}\".format(self.o_delta.shape))\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.W2.T) # z2 error: how much our hidden layer weights contributed to output error\n",
    "        #print(\"z2_error: {0}\".format(self.z2_error.shape))\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n",
    "        #print(\"z2_delta: {0}, {1}\".format(self.z2_delta.shape, self.z2_delta))\n",
    "        \n",
    "        self.W1 += X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
    "        #tmp = X.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
    "        #print(\"tmp: {0}\".format(tmp.shape))\n",
    "        #print(\"W1: {0}\".format(self.W1.shape))\n",
    "        self.W2 += self.z2.T.dot(self.o_delta) # adjusting second set (hidden --> output) weights\n",
    "        #print(\"W2: {0}\".format(self.W2.shape))\n",
    "   \n",
    "\n",
    "    def train (self, X, y):\n",
    "      o = self.forward(X)\n",
    "      self.backward(X, y, o)\n",
    "\n",
    "    def saveWeights(self):\n",
    "        np.savetxt(\"w1.txt\", self.W1, fmt=\"%s\")\n",
    "        np.savetxt(\"w2.txt\", self.W2, fmt=\"%s\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        print(\"Predicted data based on trained weights: \")\n",
    "        print(\"Input: \\n\" + str(x))\n",
    "        print(\"Output: \\n\" + str(self.forward(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our network on a first set of parameters.  Given the number 1, predict the number 2.\n",
    "\n",
    "We'll create our input and output values as vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array([1])\n",
    "\n",
    "y= np.array([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Neural_Network()\n",
    "\n",
    "#defining our output\n",
    "o = NN.forward(X)\n",
    "\n",
    "print(\"Predicted Output: \\n\" + str(o))\n",
    "print(\"Actual Output: \\n\" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1000): # trains the NN 1,000 times\n",
    "  print (\"Input: \\n\" + str(X))\n",
    "  print (\"Actual Output: \\n\" + str(y))\n",
    "  print (\"Predicted Output: \\n\" + str(NN.forward(X)))\n",
    "  print (\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss\n",
    "  print (\"\\n\")\n",
    "  NN.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ugh and another syntax error on the nature of our variables.\n",
    "\n",
    "Hand working this code a simple picture of NN with one input neuron, 3 hidden neurons, and 1 output neuron suggests the code should work.\n",
    "\n",
    "On the forward pass we multipy our 1x1 input X by W1, a 1x3 weight matrix (the inner dimensions match).  Then we multiply our 1x3 hidden layer matrix by a 3x1 weight matrix W2 to get a 1x1 output, as expected.  \n",
    "\n",
    "So, why are we seeing a dimension miss-match error on our backpropagation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Really understand numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we can create a numpy array and print it's value and compute it's own dot product using the .T transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1, 2, 3]])\n",
    "print(\"{0}\".format(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(a, a.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create random initialized weight matricies like in the init() method that match our expected dimensions for the matrix multiplies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.randn(1, 3)\n",
    "W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = np.random.randn(3,1)\n",
    "W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn(1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these seem like legitimate variables that will match the expected matrix operations during foward and back propagation. \n",
    "\n",
    "What's going on with our input?\n",
    "\n",
    "(Lots of time spent debugging the code with print statements to observe the shapes of the variables used -- the shapes appear to match.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how to create vectors in numpy.\n",
    "\n",
    "The [scipy docs provide a consicise overview of numpy array creation](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.creation.html) but don't really speak in terms of vectors and matrix operations.\n",
    "\n",
    "A little more searching on [numpy vector syntax](https://www.google.com/search?q=numpy+vector+syntax) leads to decent tutorial from IBM that compares numpy arrays and their use in matrix operations from Octave and Matlab.  Both of those environments provide very intuitive and expressive formats for specifying and manipulating vectors and matricies.  Because they provide an even higher level abstraction, they gloss over something that remains explicit in numpy: the difference between a vector and a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numpy a vector is created with a numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array([1, 2, 3])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are 1-dimensional object in Python. The shape hints at that.  Here we a have a 3d vector, which has 3 entries as shown by the .shape method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a two dimensional matrix by explicitly defining the second dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[None, :]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a 1x3 \"matrix\".  And, as you can see comparing the last result with our initial definition of X, double nested brackets distinguish a matrix versus defining the original vector. A 1x3 matrix contains one 3d vector.\n",
    "\n",
    "The shape now also returns values for both dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with our initial run using the Neural_Network object is that we passed it numpy vectors as input and not the expected numpy matrix format, which is needed to properly compute the back propagation operations.\n",
    "\n",
    "A [summary of differences between Matlab/Octave matrix implementations](https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html) and syntax helps round out our understanding and further highlights the subtleties of working with the lower-level representation of structure in numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning to our neural network training\n",
    "\n",
    "Now that we know our code wants to work with matrix input and outputs, let's create our data with using the double-bracket notation to explicitly create the 1x1 input and output matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= np.array([[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= np.array([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(X, W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(X, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Neural_Network()\n",
    "\n",
    "#defining our output\n",
    "o = NN.forward(X)\n",
    "\n",
    "print(\"Predicted Output: \\n\" + str(o))\n",
    "print(\"Actual Output: \\n\" + str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1000): # trains the NN 1,000 times\n",
    "  print (\"Input: \\n\" + str(X))\n",
    "  print (\"Actual Output: \\n\" + str(y))\n",
    "  print (\"Predicted Output: \\n\" + str(NN.forward(X)))\n",
    "  print (\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss\n",
    "  print (\"\\n\")\n",
    "  NN.train(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a neural network that takes a number as it's input and learns the weights to predict an output.\n",
    "\n",
    "Notice that our network loss approaches 1 and doesn't go beyond that value. This actually indicates a flaw with our network.\n",
    "\n",
    "Let's see what happens if we try to predict the next value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.predict([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.predict([[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our trained network always predicts the value 1.  Even though we told it a thousand times over that the next value after 1 is 2 it couldn't learn that, much less how to add one to a number to get to the next value.\n",
    "\n",
    "Clearly, our magical neural network is not very smart.\n",
    "\n",
    "Looks like it's time to explore the construction of the network to see if there's anything we could change to make it smarter: training data, dimension of input or output, or size of the hidden layer."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
